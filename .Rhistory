Lower = lower_quantile,
Upper = upper_quantile
)
return(ci_data)
}
#' Plot Fitted Logistic Curve
#'
#' Function to plot the fitted logistic curve.
#'
#' @param X Matrix of predictors
#' @param beta Coefficient vector
#' @param y Binary response variable (0 or 1)
#' @export
plot_logistic_curve = function(X, beta, y) {
# Generate sequence of values for Xbeta
x_seq <- seq(min(X %*% beta), max(X %*% beta), length.out = 100)
# Calculate predicted probabilities
pi_seq <- 1 / (1 + exp(-x_seq))
# Plot logistic curve
plot(X %*% beta, y, pch = 16, col = "black", xlab = "Xbeta", ylab = "Probability")
lines(x_seq, pi_seq, col = "blue", lwd = 2)
legend("topright", legend = "Fitted Logistic Curve", col = "blue", lwd = 2)
}
# Add more functions for confusion matrix, evaluation metrics, and plotting metrics over a grid of cutoff values.
set.seed(123)  # Setting seed for reproducibility
n <- 200
p <- 2
X <- matrix(rnorm(n * p), ncol = p)
true_beta <- c(1, -0.5)
pi <- 1 / (1 + exp(-X %*% true_beta))
y <- rbinom(n, 1, pi)
beta_hat <- logistic_regression(X, y)
print(beta_hat)
ci <- bootstrap_conf_intervals(X, y)
print(ci)
plot_logistic_curve(X, beta_hat, y)
##
initialbeta <- function(X, y) {
solve(t(X)%*%X)%*%t(X)%*%y
}
optimizing <- function(X, y, beta) {
n = nrow(X)
prob <- c()
b <- c()
n = length(y)
for (i in 1:n){
prob[i] <- 1/1+exp(t(-X[i,])%*%beta)
}
for (i in 1:n){
b[i] <- (-y[i]*log(prob[i])-(1-y[i])*log(1-prob[i]))
}
return(sum(b))
}
estBeta <- function(X, y) {
betahat <- initialbeta(X, y)
betan <- optim(betahat, optimizing, X, y)
result <- list(betahat,betan)
return(result)
}
interval <- function(X, y, alpha, B = 20){
n = nrow(X)
p = ncol(X)
beta_B = matrix(0, p, B)
for (b in 1:B) {
ind = sample(1:n,n,replace = TRUE)
X_b = X[,ind]
y_b = y[ind]
beta_B[, b] = estBeta(X_b, y_b)
}
upper = apply(beta_B, 1, quantile, 1-alpha)
lower = apply(beta_B, 1, quantile, alpha)
beta_ci = cbind(lower,upper)
return(beta_ci)
}
##
initialbeta <- function(X, y) {
solve(t(X)%*%X)%*%t(X)%*%y
}
optimizing <- function(X, y, beta) {
n = nrow(X)
prob <- c()
b <- c()
n = length(y)
for (i in 1:n){
prob[i] <- 1/1+exp(t(-X[i,])%*%beta)
}
for (i in 1:n){
b[i] <- (-y[i]*log(prob[i])-(1-y[i])*log(1-prob[i]))
}
return(sum(b))
}
estBeta <- function(X, y) {
betahat <- initialbeta(X, y)
betan <- optim(betahat, optimizing, X, y)
result <- list(betahat,betan)
return(result)
}
#Bootstrap Confidence intervals:
interval <- function(X, y, alpha, B = 20){
n = nrow(X)
p = ncol(X)
beta_B = matrix(0, p, B)
for (b in 1:B) {
ind = sample(1:n,n,replace = TRUE)
X_b = X[,ind]
y_b = y[ind]
beta_B[, b] = estBeta(X_b, y_b)
}
upper = apply(beta_B, 1, quantile, 1-alpha)
lower = apply(beta_B, 1, quantile, alpha)
beta_ci = cbind(lower,upper)
return(beta_ci)
}
#Plot of the fitted logistic curve to the actual values.
logiplot <- function(X, y, beta) {
fitmd <- optimizing(X, y, beta)
predata <- data.frame(X = seq(min(X), max(X), len = 1000))
yhat = predict(fitmd, predata, type = "response")
plot(y ~ X, col = "green", main = "Logistic Curve", xlab = "X", ylab = "p")
lines(y ~ X, yhat, lwd = 2, col = "blue")
}
# Confusion Matrix
confusion_matrix <- function(X, y, bhat, cut = 0.5){
yhat = (1/(1+exp(-X%*%bhat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
conf_mat = cbind(c(tp, fp), c(fn, tn))
return(conf_mat)
}
Prevalence <- function(X, y, bhat, cut = 0.5){
yhat = (1/(1+exp(-X%*%bhat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
Prevalence = (fn+tp)/(tp+fp+tn+fn)
return(Prevalence)
}
Accuracy <- function(X, y, bhat, cut = 0.5){
yhat = (1/(1+exp(-X%*%bhat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
Accuracy = (tn+tp)/(tp+fp+tn+fn)
return(Accuracy)
}
Sensitivity <- function(X, y, bhat, cut = 0.5){
yhat = (1/(1+exp(-X%*%bhat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
Sensitivity = tp/(tp+fn)
return(Sensitivity)
}
Specificity <- function(X, y, bhat, cut = 0.5){
yhat = (1/(1+exp(-X%*%bhat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
Specificity = tn/(tn+fp)
return(Specificity)
}
False_Discovery_Rate <- function(X, y, bhat, cut = 0.5){
yhat = (1/(1+exp(-X%*%bhat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
False_Discovery_Rate= fp/(tp+fp)
return(False_Discovery_Rate)
}
Diagnostic_Odds_ration <- function(X, y, bhat, cut = 0.5){
yhat = (1/(1+exp(-X%*%bhat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
Diagnostic_Odds_ration = (sensitivity / False_Discovery_Rate) / (False_Discovery_Rate/ specificity)
return(Diagnostic_Odds_ration)
}
prevalencegrid <- function(X, y, bhat, cut = 0.5){
yhat = 1/(1+exp(-X%*%bhat))
ifelse(yhat > 0.5,1,0)
tp = sum((yhat == 1)&(y == 1))
fp = sum((yhat == 0)&(y == 1))
tn = sum((yhat == 0)&(y == 0))
fn = sum((yhat == 1)&(y == 0))
Prevalence = (fn+tp)/(tp+fp+tn+fn)
cut<-seq(0.1, 0.9, by = 0.1)
Prevalence <- matrix(NA, 9, 1)
for (i in 1:9){
value <- cut[i]
Prevalence[i,] <- matrix(X, y, value)
}
return(Prevalence)
}
#' Bootstrap Confidence Intervals
#'
#' Function to calculate bootstrap confidence intervals for logistic regression coefficients.
#'
#' @param X Matrix of predictors
#' @param y Binary response variable (0 or 1)
#' @param alpha Significance level for confidence intervals
#' @param n_bootstraps Number of bootstrap samples
#' @return Bootstrap confidence intervals for each coefficient
#' @export
bootstrap_conf_intervals <- function(X, y, alpha = 0.05, n_bootstraps = 20) {
# Get the number of observations and predictors
n <- nrow(X)
p <- ncol(X)
# Initialize an empty matrix to store bootstrap samples
bootstrap_samples <- matrix(0, nrow = n_bootstrap, ncol = p)
# Perform bootstrap sampling
set.seed(100)  # Set seed for reproducibility
for (i in 1:n_bootstrap) {
# Sample with replacement
bootstrap_indices <- sample(1:n, replace = TRUE)
bootstrap_X <- X[bootstrap_indices, ]
bootstrap_y <- y[bootstrap_indices]
# Fit logistic regression on the bootstrap sample
fit <- logistic_regression(bootstrap_X, bootstrap_y)
# Store the coefficients in the matrix
bootstrap_samples[i, ] <- fit
}
# Compute quantiles for confidence intervals
lower_quantile <- quantile(bootstrap_samples, alpha / 2)
upper_quantile <- quantile(bootstrap_samples, 1 - alpha / 2)
# Create a data frame with coefficients and confidence intervals
ci_data <- data.frame(
Coefficient = colnames(bootstrap_samples),
Estimate = colMeans(bootstrap_samples),
Lower = lower_quantile,
Upper = upper_quantile
)
return(ci_data)
}
#' @export
bootstrap_conf_intervals = function(X, y, alpha = 0.05, n_bootstraps = 20) {
n = nrow(X)
p = ncol(X)
# Initialize an empty matrix to store bootstrap samples
b_samples = matrix(0, p, n_bootstraps)
# Perform bootstrap sampling
set.seed(100)  # Set seed for reproducibility
for (b in 1:n_bootstraps) {
# Sample with replacement
b_indices = sample(1:n, n, replace = TRUE)
b_X = X[b_indices,]
b_y = y[b_indices]
b_samples[, b] = logistic_regression(b_X, b_y)
}
# Compute quantiles for confidence intervals
lower = apply(b_samples, 1, quantile, 1-alpha)
upper = apply(b_samples, 1, quantile, alpha)
ci = cbind(lower, upper)
return(ci)
}
@@ -80,26 +69,217 @@ bootstrap_conf_intervals <- function(X, y, alpha = 0.05, n_bootstraps = 20) {
#' Function to plot the fitted logistic curve.
#'
#' @param X Matrix of predictors
#' @param beta_hat Coefficient vector
#' @param y Binary response variable (0 or 1)
#' @export
plot_logistic_curve = function(X, beta_hat, y) {
# Generate sequence of values for Xbeta
x_seq = seq(min(X %*% beta_hat), max(X %*% beta_hat), length.out = 100)
# Calculate predicted probabilities
pi_seq = 1 / (1 + exp(-x_seq))
# Plot logistic curve
plot(X %*% beta_hat, y, pch = 16, col = "black", xlab = "Xbeta", ylab = "Probability")
lines(x_seq, pi_seq, col = "blue", lwd = 2)
}
#' Confusion Matrix
#'
#' Confusion matrix with a cut-off value for predictions at 0.5. Predictions above 0.5 are assigned a 1, below 0.5 are assigned 0.
#'
#' @param X Matrix of predictors
#' @param beta_hat Coefficient vector
#' @param y Binary response variable (0 or 1)
#' @param cut Cut off value
#' @return Confidence Matrix
#' @export
confusion_matrix = function(X, y, beta_hat, cut = 0.5){
yhat = (1/(1+exp(-X%*%beta_hat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
conf_mat = cbind(c(tp, fp), c(fn, tn))
return(conf_mat)
}
#' Prevalence
#'
#' Calculates the prevalence based on the confusion matrix.
#'
#' @param X Matrix of predictors
#' @param y Binary response variable (0 or 1)
#' @param beta_hat Coefficient vector
#' @param cut Cut off value
#' @return Prevalence
#' @export
Prevalence = function(X, y, beta_hat, cut = 0.5){
yhat = (1/(1+exp(-X%*%beta_hat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
Prevalence = (fn+tp)/(tp+fp+tn+fn)
return(Prevalence)
}
#' Accuracy
#'
#' Calculates the accuracy based on the confusion matrix.
#'
#' @param X Matrix of predictors
#' @param y Binary response variable (0 or 1)
#' @param beta_hat Coefficient vector
#' @param cut Cut off value
#' @return Accuracy
#' @export
Accuracy = function(X, y, beta_hat, cut = 0.5){
yhat = (1/(1+exp(-X%*%beta_hat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
Accuracy = (tn+tp)/(tp+fp+tn+fn)
return(Accuracy)
}
#' Sensitivity
#'
#' Calculates the sensitivity based on the confusion matrix.
#'
#' @param X Matrix of predictors
#' @param y Binary response variable (0 or 1)
#' @param beta_hat Coefficient vector
#' @param cut Cut off value
#' @return Sensitivity
#' @export
Sensitivity = function(X, y, beta_hat, cut = 0.5){
yhat = (1/(1+exp(-X%*%beta_hat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
Sensitivity = tp/(tp+fn)
return(Sensitivity)
}
#' Specificity
#'
#' Calculates the specificity based on the confusion matrix.
#'
#' @param X Matrix of predictors
#' @param y Binary response variable (0 or 1)
#' @param beta_hat Coefficient vector
#' @param cut Cut off value
#' @return Specificity
#' @export
Specificity = function(X, y, beta_hat, cut = 0.5){
yhat = (1/(1+exp(-X%*%beta_hat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
Specificity = tn/(tn+fp)
return(Specificity)
}
#' False Discovery Rate
#'
#' Calculates the false discovery rate based on the confusion matrix.
#'
#' @param X Matrix of predictors
#' @param y Binary response variable (0 or 1)
#' @param beta_hat Coefficient vector
#' @param cut Cut off value
#' @return False Discovery Rate
#' @export
False_Discovery_Rate = function(X, y, beta_hat, cut = 0.5){
yhat = (1/(1+exp(-X%*%beta_hat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
False_Discovery_Rate = fp/(tp+fp)
return(False_Discovery_Rate)
}
#' Diagnostic Odds Ratio
#'
#' Calculates the diagnostic odds ratio based on the confusion matrix.
#'
#' @param X Matrix of predictors
#' @param y Binary response variable (0 or 1)
#' @param beta_hat Coefficient vector
#' @param cut Cut off value
#' @return Diagnostic Odds Ratio
#' @export
Diagnostic_Odds_ratio = function(X, y, beta_hat, cut = 0.5){
yhat = (1/(1+exp(-X%*%beta_hat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
Diagnostic_Odds_ratio = (Sensitivity(X,y,beta_hat) / False_Discovery_Rate(X,y,beta_hat)) / (False_Discovery_Rate(X,y,beta_hat)/ Specificity(X,y,beta_hat))
return(Diagnostic_Odds_ratio)
}
#' Prevalence Grid
#'
#' Calculates prevalence for different cut-off values and returns a matrix.
#'
#' @param X Matrix of predictors
#' @param y Binary response variable (0 or 1)
#' @param beta_hat Coefficient vector
#' @param cut Vector of cut-off values
#' @return Matrix of prevalence values for each cut-off
#' @export
prevalencegrid <- function(X, y, beta_hat, cut = seq(0.1, 0.9, by = 0.1)) {
n_cuts = length(cut)
metrics_matrix = matrix(NA, n_cuts, 7,
dimnames = list(NULL, c("Cut-Off", "Prevalence", "Accuracy", "Sensitivity", "Specificity", "False_Discovery_Rate", "Diagnostic_Odds_Ratio")))
for (i in 1:n_cuts) {
current_cut = cut[i]
yhat = (1 / (1 + exp(-X %*% beta_hat)) > current_cut) * 1
metrics_matrix[i, 1] = current_cut
metrics_matrix[i, 2] = Prevalence(X, y, beta_hat, current_cut)
metrics_matrix[i, 3] = Accuracy(X, y, beta_hat, current_cut)
metrics_matrix[i, 4] = Sensitivity(X, y, beta_hat, current_cut)
metrics_matrix[i, 5] = Specificity(X, y, beta_hat, current_cut)
metrics_matrix[i, 6] = False_Discovery_Rate(X, y, beta_hat, current_cut)
metrics_matrix[i, 7] = Diagnostic_Odds_ratio(X, y, beta_hat, current_cut)
}
return(metrics_matrix)
}
# Testing functions
set.seed(123)  # Setting seed for reproducibility
n <- 200
p <- 2
@@ -117,4 +297,11 @@ ci <- bootstrap_conf_intervals(X, y)
print(ci)
plot_logistic_curve(X, beta_hat, y)
confusion_matrix(X, y, beta_hat)
Prevalence(X, y, beta_hat)
Accuracy(X, y, beta_hat)
Sensitivity(X, y, beta_hat)
Specificity(X, y, beta_hat)
False_Discovery_Rate(X, y, beta_hat)
Diagnostic_Odds_ratio(X, y, beta_hat)
prevalencegrid(X, y, beta_hat)
devtools::document()
#' Prevalence
#'
#' Calculates the prevalence based on the confusion matrix.
#'
#' @param X Matrix of predictors
#' @param y Binary response variable (0 or 1)
#' @param beta_hat Coefficient vector
#' @param cut Cut off value
#' @return Prevalence
#' @export
Prevalence = function(X, y, beta_hat, cut = 0.5){
yhat = (1/(1+exp(-X%*%beta_hat)) > 0.5)*1
tp = sum((yhat==1)&(y==1))
fp = sum((yhat==0)&(y==1))
tn = sum((yhat==0)&(y==0))
fn = sum((yhat==1)&(y==0))
Prevalence = (fn+tp)/(tp+fp+tn+fn)
return(Prevalence)
}
devtools::document()
devtools::document()
rm(list = c("Accuracy", "bootstrap_conf_intervals", "confusion_matrix", "Diagnostic_Odds_ratio",
"False_Discovery_Rate", "logistic_regression", "plot_logistic_curve", "Prevalence", "prevalencegrid", "Sensitivity",
"Specificity"))
devtools::document()
rmarkdown::render("README.Rmd", output_file = "F:/Fall 2023 AU/R Programming for Data Science/README.html")
rmarkdown::render("README.Rmd", output_file = "F:/Fall 2023 AU/R Programming for Data Science/fp.html")
install.packages("devtools")
install.packages("devtools")
library(devtools)
install_github(" R-4-Data-Science/FinalProjGroup10", build_vignettes = TRUE)
install_github("R-4-Data-Science/FinalProjGroup10")
install_github("R-4-Data-Science/FinalProjGroup10",build_vignettes = TRUE)
install.packages("p1")
install_github("R-4-Data-Science/FinalProjGroup10",build_vignettes = TRUE)
force = TRUE
devtools::install_local()
install_github("p1",build_vignettes = TRUE)
R-4-Data-Science/FinalProjGroup10
install_github("R-4-Data-Science/FinalProjGroup10",build_vignettes = TRUE)
vignette(package = "p1")
# To open a specific vignette
vignette("vignette_name", package = "p1")
vignette(package = "p1")
vignette("vignette_name", package = "p1")
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
vignette(package = "p1")
devtools::build_vignettes()
---
title: "Final_Project_Group_10"
output: rmarkdown::html_vignette
vignette: >
%\VignetteIndexEntry{Vignette Title}
%\VignetteEngine{knitr::rmarkdown}
\usepackage[utf8]{inputenc}
---
